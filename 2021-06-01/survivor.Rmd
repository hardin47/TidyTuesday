---
title: "SurvivoR"
author: "Jo Hardin"
date: "6/1/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE,
                      fig.width=10, eval.after = 'fig.cap')
library(tidyverse)
library(infer)
library(broom)
library(tidymodels)
library(lubridate)
library(dotwhisker)

library(praise)
set.seed(4747)
```

### The Data


The data this week comes from the [`survivorR` R package](https://github.com/doehm/survivoR) by way of Daniel Oehm.

```{r}
castaways <- read_csv("castaways.csv") %>%
  select(season, castaway, age, state, personality_type, order, 
         total_votes_received, immunity_idols_won) %>%
  drop_na()
```


|variable             |class     |description |
|:--------------------|:---------|:-----------|
|season          |integer   | Season number |
|castaway             |character | Castaway's first name |
|age                  |double    | Age |
|state                |character | Origin state |
|personality_type     |character | personality type |
|order                |integer   | Order |
|total_votes_received |double    | Total votes received |
|immunity_idols_won   |double    | Immunity idols won |


### Tidy Models

For the next few weeks, the goal of my Tidy Tuesday analyses will be to get used to working with **tidymodels**.  This week, we'll try to predict which week (`order`) the individual was cast out using a few of the variables.  Note that votes will be a good measure of prediction which would silly to use in a real model, but here the idea is just to practice using tidymodels.

1. Break the data into test and training
2. Run a linear model
3. Run a random forest
4. Use RMSE to evaluate the two models

Note after the fact:  lots of learning today, and I'm starting to get the hang of the tidymodels framework.  However, the dataset doesn't have very many variables.  I look forward to next week's data in hopes that I'll be able to model with more variables.


####  Test / train data

We'll want to first split the data into test and training sets.

```{r}
set.seed(47)

data_split <- initial_split(castaways, prop = 2/3)

cast_train <- training(data_split)
cast_test <- testing(data_split)
```

We'll also specify the `recipe()` we want to use.  We'll use the same recipe for both the linear model and the random forest.  The power of the `recipe()` function is all about feature selection.  There are lots of ways to build / combine / edit features for improved prediction.  We won't do that here.

```{r}
cast_rec <-
  recipe(order ~ ., data = cast_train) %>%
  update_role(season, castaway, state, new_role = "ID") #%>%
  #step_dummy(all_nominal(), -all_outcomes())

summary(cast_rec)
```


#### Linear model

As specified in the [tidymodels vignette](https://www.tidymodels.org/) (I'll be using it heavily to work through today's analysis), we set the engine to be a linear regression model.

```{r}
lm_mod <-
  linear_reg() %>%
  set_engine("lm")
```

After we've specified the engine, we can then `fit()` the model:

```{r}
cast_lm <-
  workflow() %>%
  add_model(lm_mod) %>%
  add_recipe(cast_rec) %>%
  fit(data = cast_train)

cast_lm %>% tidy()
```

Let's look at the coefficients:

```{r}
tidy(cast_lm) %>%
  dwplot() +
  geom_vline(xintercept = 0)
```

#### Random forest

The recipe is already set, so now, the only thing that needs to change is the engine which builds the model.  Note, however, that with a random forest, it'll be important to specify the tuning parameters (which don't exist in a linear model).  

```{r}
rf_mod1 <-
  rand_forest(mtry = 3,
              trees = 500,
              min_n = 5) %>%
  set_engine("ranger", importance = "permutation") %>%
  set_mode("regression") 
```


After we've specified the engine, we can then `fit()` the model:

```{r}
cast_rf1 <-
  workflow() %>%
  add_model(rf_mod1) %>%
  add_recipe(cast_rec) %>%
  fit(data = cast_train)

cast_rf1
```

```{r}
library(vip)

cast_rf1 %>%
  pull_workflow_fit() %>%
  vip()
```

### CV to find a better values of `mtry` and `min_n` in the random forest.

```{r}
rf_mod2 <-
  rand_forest(mtry = tune(),
              trees = 500,
              min_n = tune()) %>%
  set_engine("ranger", importance = "permutation") %>%
  set_mode("regression") 
```

After we've specified the engine, we can then `fit()` the model:

```{r}
cast_rf2 <-
  workflow() %>%
  add_model(rf_mod2) %>%
  add_recipe(cast_rec) %>%
  fit(data = cast_train)

cast_rf2
```


Now we'll CV the data so that we can run the random forest on different subsets of the training data.

```{r}
set.seed(4747)
cast_folds <- vfold_cv(cast_train, v = 4)

cast_grid <- grid_regular(mtry(range = c(1,3)),
                          min_n(range = c(5,10)),
                          levels = 3)

cast_grid
```

Now we tune the model, this time using the grid of parameter values.

```{r}
doParallel::registerDoParallel()

set.seed(470)
tune_result <- tune_grid(
  cast_rf2,
  resamples = cast_folds,
  grid = cast_grid
)

tune_result
```



```{r}
tune_result %>%
  collect_metrics() %>%
  filter(.metric == "rmse") %>%
  mutate(min_n = factor(min_n)) %>%
  ggplot(aes(x = mtry, y = mean, color = min_n)) +
  geom_line(alpha = 0.5, size = 1.5) +
  geom_point() + 
  labs(y = "RMSE")
```


### Use RMSE to evaluate the models


First we look at the linear model used to predict the test data.

```{r}
cast_lm_final <-
  cast_lm %>%
  last_fit(data_split)

cast_lm_final %>%
  collect_metrics()
```

Similarly, we can look at the random forest fit.  The random forest did slightly better on the test data.

```{r}
cast_rf_final <-
  cast_rf1 %>%
  last_fit(data_split)

cast_rf_final %>%
  collect_metrics()
```



```{r}
praise()
```